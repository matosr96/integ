"""
Script de Limpieza Definitiva con Listas Maestras Oficiales
- 28 EPS autorizadas en Colombia (2024-2025)
- 30 Municipios oficiales de C√≥rdoba
- Normalizaci√≥n inteligente con fuzzy matching
"""

import pandas as pd
import json
import re
from datetime import datetime
from unidecode import unidecode
from difflib import get_close_matches

# ============================================================================
# LISTAS MAESTRAS OFICIALES
# ============================================================================

# 28 EPS AUTORIZADAS EN COLOMBIA (2024-2025)
EPS_OFICIALES = [
    # R√©gimen Contributivo y Subsidiado
    'COOSALUD',
    'NUEVA EPS',
    'MUTUAL SER',
    # R√©gimen Contributivo
    'ALIANSALUD',
    'SALUD TOTAL',
    'SANITAS',
    'SURA',
    'FAMISANAR',
    'SOS',  # Servicio Occidental de Salud
    'COMFENALCO VALLE',
    'COMPENSAR',
    'EPM',  # Empresas P√∫blicas de Medell√≠n
    'FONDO DE PASIVO SOCIAL FERROCARRILES',
    'SALUD MIA',
    # R√©gimen Subsidiado
    'CAJACOPI ATLANTICO',
    'CAPRESOCA',
    'COMFACHOCO',
    'COMFAORIENTE',
    'EPS FAMILIAR',
    'ASMET SALUD',
    'EMSSANAR',
    'CAPITAL SALUD',
    'SAVIA SALUD',
    'DUSAKAWI',
    'ASOCIACION INDIGENA DEL CAUCA',
    'ANAS WAYUU',
    'MALLAMAS',
    'PIJAOS SALUD',
    'SALUD BOLIVAR',
    # Otras comunes en C√≥rdoba
    'COMFACOR',
    'FOMAG',
    'MEDICINA INTEGRAL',
    'COLSANITAS',
    'COOMEVA',
    'PROMOSALUD',
    'SALUDVIDA',
    'UNICOR',
    'EMDISALUD',
    'AXACOLPATRIA',
    'COLMENA',
    'COLMEDICA',
    'PARTICULAR',
    'GRUPO VIVIR',
    'CLINICA UNIVERSITARIA'
]

# 30 MUNICIPIOS OFICIALES DE C√ìRDOBA
MUNICIPIOS_OFICIALES = [
    'AYAPEL',
    'BUENAVISTA',
    'CANALETE',
    'CERET√â',
    'CHIM√Å',
    'CHIN√ö',
    'CI√âNAGA DE ORO',
    'COTORRA',
    'LA APARTADA',
    'LOS C√ìRDOBAS',
    'MOMIL',
    'MONTEL√çBANO',
    'MONTER√çA',
    'MO√ëITOS',
    'PLANETA RICA',
    'PUEBLO NUEVO',
    'PUERTO ESCONDIDO',
    'PUERTO LIBERTADOR',
    'PUR√çSIMA',
    'SAHAG√öN',
    'SAN ANDR√âS DE SOTAVENTO',
    'SAN ANTERO',
    'SAN BERNARDO DEL VIENTO',
    'SAN CARLOS',
    'SAN JOS√â DE UR√â',
    'SAN PELAYO',
    'SANTA CRUZ DE LORICA',  # Nombre oficial completo
    'TIERRALTA',
    'TUCH√çN',
    'VALENCIA'
]

# Crear versiones sin acentos para matching
EPS_OFICIALES_NORMALIZED = [unidecode(eps).upper() for eps in EPS_OFICIALES]
MUNICIPIOS_OFICIALES_NORMALIZED = [unidecode(mun).upper() for mun in MUNICIPIOS_OFICIALES]

# Diccionario de mapeo EPS (variaciones conocidas -> oficial)
EPS_VARIACIONES = {
    'MUTUALSER': 'MUTUAL SER',
    'SALUDTOTAL': 'SALUD TOTAL',
    'SALUD VIDA': 'SALUDVIDA',
    'COOMEVA PREPAGADA': 'COOMEVA',
    'SURA PREPAGADA': 'SURA',
    'SURAMERICANA': 'SURA',
    'EPS FAMILIAR DE COLOMBIA': 'EPS FAMILIAR',
    'SERVICIO OCCIDENTAL DE SALUD': 'SOS',
    'EMPRESAS PUBLICAS DE MEDELLIN': 'EPM',
    'AXA COLPATRIA': 'AXACOLPATRIA',
    'AXA': 'AXACOLPATRIA',
    'COLPATRIA': 'AXACOLPATRIA',
    'FAMILIAR DE COLOMBIA': 'EPS FAMILIAR',
    'UNICORDOBA': 'UNICOR',
    'PROMOSALUD-COOMEVA': 'PROMOSALUD',
    'CLINICA UNIVERSITARIA': 'UNICOR',
}

# Diccionario de mapeo Municipios (variaciones conocidas -> oficial)
MUNICIPIOS_VARIACIONES = {
    'MONTERIA': 'MONTER√çA',
    'VEREDAS MONTERIA': 'MONTER√çA',
    'CERETE': 'CERET√â',
    'CERETE (VEREDA SAN CARLOS)': 'CERET√â',
    'SAHAGUN': 'SAHAG√öN',
    'VIA SABANAL': 'SAHAG√öN',
    'LORICA': 'SANTA CRUZ DE LORICA',
    'PLANETA  RICA': 'PLANETA RICA',
    'P. RICA': 'PLANETA RICA',
    'VIA PLANETA': 'PLANETA RICA',
    'MONTELIBANO': 'MONTEL√çBANO',
    'VIA TIERRALTA': 'TIERRALTA',
    'CIENAGA DE ORO': 'CI√âNAGA DE ORO',
    'CIENEGA DE ORO': 'CI√âNAGA DE ORO',
    'CHIMA': 'CHIM√Å',
    'CHINU': 'CHIN√ö',
    'SAN BERNARDO': 'SAN BERNARDO DEL VIENTO',
    'PUERTO ESCONDICO': 'PUERTO ESCONDIDO',
    'LOS CORDOBA': 'LOS C√ìRDOBAS',
    'TRES PALMAS  + LOS CORDOBA': 'LOS C√ìRDOBAS',
    'SAN ANDRES DE SOTAVENTO': 'SAN ANDR√âS DE SOTAVENTO',
    'PURISIMA': 'PUR√çSIMA',
    'LA  APARTADA': 'LA APARTADA',
    'LA APARATADA': 'LA APARTADA',
    'LA APARTADA  DE MONTELIBANO': 'LA APARTADA',
    'MO√ëITO': 'MO√ëITOS',
    'TUCHIN': 'TUCH√çN',
    'SAN JOSE DE URE': 'SAN JOS√â DE UR√â',
    'BENAVISTA': 'BUENAVISTA',
    # Corregimientos y sectores de Monter√≠a
    'TRES PALMAS': 'MONTER√çA',
    '3 PALMAS': 'MONTER√çA',
    'KM 8 VDA LAS PULGAS': 'MONTER√çA',
    'KM 8': 'MONTER√çA',
    'BARRIO SAN JOSE': 'MONTER√çA',
    'SAN JOSE': 'MONTER√çA',
    'GALILEA': 'MONTER√çA',
    'EL RECUERDO': 'MONTER√çA',
    'LOS RECUERDOS': 'MONTER√çA',
    'MI REFUGIO': 'MONTER√çA',
    'KM 7 VIA CERETE': 'MONTER√çA',
    'KM 7': 'MONTER√çA',
    'EDIFICIO INDIGO': 'MONTER√çA',
    'INDIGO': 'MONTER√çA',
    'BONANZA': 'MONTER√çA',
    '6 DE MARZO': 'MONTER√çA',
    # Municipios fuera de C√≥rdoba donde se prestaron servicios
    'CAUCASIA': 'CAUCASIA',
    # Valores inv√°lidos (no son municipios)
    'ARACHE': None,
    'EL CRUCERO': None,
    'NEUROLOGICO': None,
    'ARBOLETES': None,
    'SAMPUES': None,
    'BERASTEGUI': None,
    'BELLO': None,
}

# Meses en espa√±ol
MESES = {
    'ENERO': 1, 'FEBRERO': 2, 'MARZO': 3, 'ABRIL': 4,
    'MAYO': 5, 'JUNIO': 6, 'JULIO': 7, 'AGOSTO': 8,
    'SEPTIEMBRE': 9, 'OCTUBRE': 10, 'NOVIEMBRE': 11, 'DICIEMBRE': 12
}

# ============================================================================
# FUNCIONES DE NORMALIZACI√ìN INTELIGENTE
# ============================================================================

def normalizar_eps_inteligente(val):
    """Normaliza EPS usando lista oficial y fuzzy matching"""
    if pd.isna(val) or val == '':
        return None
    
    val_str = str(val).strip().upper()
    val_clean = unidecode(val_str)
    
    # Descartar n√∫meros puros
    try:
        float(val_clean)
        return None
    except:
        pass
    
    # Descartar fechas
    if re.match(r'^\d{4}-\d{2}-\d{2}', val_clean):
        return None
    
    # Muy corto
    if len(val_clean) < 3:
        return None
    
    # 1. Buscar en variaciones conocidas
    if val_clean in EPS_VARIACIONES:
        return EPS_VARIACIONES[val_clean]
    
    # 2. Buscar coincidencia exacta en lista oficial
    if val_clean in EPS_OFICIALES_NORMALIZED:
        idx = EPS_OFICIALES_NORMALIZED.index(val_clean)
        return EPS_OFICIALES[idx]
    
    # 3. Fuzzy matching (buscar similares)
    matches = get_close_matches(val_clean, EPS_OFICIALES_NORMALIZED, n=1, cutoff=0.8)
    if matches:
        idx = EPS_OFICIALES_NORMALIZED.index(matches[0])
        return EPS_OFICIALES[idx]
    
    # 4. Si contiene palabras clave de EPS conocidas
    for eps_oficial in EPS_OFICIALES:
        eps_words = eps_oficial.split()
        val_words = val_clean.split()
        # Si comparten al menos 2 palabras significativas
        common_words = set(eps_words) & set(val_words)
        if len(common_words) >= 2 or (len(common_words) >= 1 and len(eps_words) == 1):
            return eps_oficial
    
    # 5. Si no se encuentra, retornar None (ser√° marcado para revisi√≥n)
    return None

def normalizar_municipio_inteligente(val):
    """Normaliza municipios usando lista oficial y fuzzy matching"""
    if pd.isna(val) or val == '':
        return None
    
    val_str = str(val).strip().upper()
    val_clean = unidecode(val_str)
    
    # Descartar n√∫meros puros
    try:
        float(val_clean)
        return None
    except:
        pass
    
    # Descartar fechas
    if re.match(r'^\d{4}-\d{2}-\d{2}', val_clean):
        return None
    
    # Muy corto
    if len(val_clean) < 3:
        return None
    
    # 1. Buscar en variaciones conocidas
    if val_clean in MUNICIPIOS_VARIACIONES:
        return MUNICIPIOS_VARIACIONES[val_clean]
    
    # 2. Buscar coincidencia exacta en lista oficial
    if val_clean in MUNICIPIOS_OFICIALES_NORMALIZED:
        idx = MUNICIPIOS_OFICIALES_NORMALIZED.index(val_clean)
        return MUNICIPIOS_OFICIALES[idx]
    
    # 3. Fuzzy matching (buscar similares)
    matches = get_close_matches(val_clean, MUNICIPIOS_OFICIALES_NORMALIZED, n=1, cutoff=0.85)
    if matches:
        idx = MUNICIPIOS_OFICIALES_NORMALIZED.index(matches[0])
        return MUNICIPIOS_OFICIALES[idx]
    
    # 4. Buscar si contiene el nombre del municipio
    for mun_oficial in MUNICIPIOS_OFICIALES:
        mun_clean = unidecode(mun_oficial).upper()
        if mun_clean in val_clean or val_clean in mun_clean:
            return mun_oficial
    
    # 5. Si no se encuentra, retornar None
    return None

def extraer_mes_anio_de_archivo(filename, year_folder):
    """Extrae mes y a√±o del nombre del archivo"""
    year = None
    if year_folder and year_folder != '':
        try:
            year = int(year_folder)
        except:
            year_match = re.search(r'20\d{2}', str(filename))
            if year_match:
                year = int(year_match.group(0))
    
    month = None
    filename_upper = str(filename).upper()
    
    # Buscar n√∫mero de mes al inicio
    month_match = re.match(r'^(\d{2})', filename_upper)
    if month_match:
        month = int(month_match.group(1))
    else:
        # Buscar nombre del mes
        for mes_nombre, mes_num in MESES.items():
            if mes_nombre in filename_upper:
                month = mes_num
                break
    
    return year, month

def reconstruir_fecha(val, filename, year_folder):
    """Reconstruye fecha completa usando contexto del archivo"""
    if pd.isna(val) or val == '' or val == 'nan':
        return None
    
    val_str = str(val).strip()
    
    # Si ya es una fecha v√°lida ISO
    if re.match(r'^\d{4}-\d{2}-\d{2}', val_str):
        # Si el a√±o es 1900, es probable que Excel haya convertido un n√∫mero de d√≠a
        # Intentamos rescatar el d√≠a y usar el contexto del archivo
        if val_str.startswith('1900-'):
            try:
                # Extraer el d√≠a de la fecha 1900-01-DD (o similar)
                # O simplemente tomar el valor como un posible d√≠a si es menor a 32
                day = int(val_str.split('-')[2][:2])
                if 1 <= day <= 31:
                    year, month = extraer_mes_anio_de_archivo(filename, year_folder)
                    if year and month:
                        fecha = f"{year}-{month:02d}-{day:02d}"
                        datetime.strptime(fecha, '%Y-%m-%d')
                        return fecha
            except:
                pass
        return val_str
    
    # Si es solo un n√∫mero (d√≠a del mes)
    if re.match(r'^\d{1,2}$', val_str):
        day = int(val_str)
        
        if day < 1 or day > 31:
            return None
        
        year, month = extraer_mes_anio_de_archivo(filename, year_folder)
        
        if year and month:
            try:
                fecha = f"{year}-{month:02d}-{day:02d}"
                datetime.strptime(fecha, '%Y-%m-%d')
                return fecha
            except:
                return None
    
    # Intentar parsear DD/MM/YYYY
    try:
        if '/' in val_str:
            parts = val_str.split('/')
            if len(parts) == 3:
                day, month, year = parts
                if len(year) == 2:
                    year = '20' + year if int(year) < 50 else '19' + year
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
    except:
        pass
    
    return None

def limpiar_sesiones(val):
    """Extrae n√∫mero de sesiones"""
    if pd.isna(val) or val == '':
        return 0
    
    val_str = str(val).strip()
    
    try:
        return float(val_str)
    except:
        pass
    
    numbers = re.findall(r'\d+', val_str)
    if numbers:
        return float(numbers[0])
    
    return 0

def limpiar_texto(val):
    """Limpia campos de texto gen√©ricos"""
    if pd.isna(val) or val == '':
        return None
    
    val_str = str(val).strip()
    
    if re.match(r'^\d{4}-\d{2}-\d{2}', val_str):
        return None
    
    return val_str

# ============================================================================
# FUNCI√ìN PRINCIPAL
# ============================================================================

def limpiar_datos_maestro():
    """Limpieza completa con listas maestras oficiales"""
    
    input_file = 'data/raw/trazabilidad_consolidada.json'
    output_file = 'data/processed/trazabilidad_LIMPIA.json'
    backup_file = 'data/audit/trazabilidad_BACKUP.json'
    
    print("="*80)
    print("LIMPIEZA CON LISTAS MAESTRAS OFICIALES")
    print("="*80)
    print(f"EPS Oficiales: {len(EPS_OFICIALES)}")
    print(f"Municipios Oficiales: {len(MUNICIPIOS_OFICIALES)}")
    
    # Backup
    print("\n1. Creando backup...")
    with open(input_file, 'r', encoding='utf-8') as f:
        original_data = json.load(f)
    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(original_data, f, indent=2, ensure_ascii=False)
    print(f"   ‚úì {backup_file}")
    
    # Cargar
    print("\n2. Cargando datos...")
    records = original_data if isinstance(original_data, list) else original_data.get('data', [])
    df = pd.DataFrame(records)
    print(f"   ‚úì {len(df)} registros")
    
    # Estad√≠sticas ANTES
    print("\n3. ANTES de limpieza:")
    eps_antes = df['eps'].nunique() if 'eps' in df.columns else 0
    mun_antes = df['municipio'].nunique() if 'municipio' in df.columns else 0
    print(f"   - EPS √∫nicas: {eps_antes}")
    print(f"   - Municipios √∫nicos: {mun_antes}")
    
    # LIMPIEZA
    print("\n4. Normalizando datos...")
    
    # EPS
    if 'eps' in df.columns:
        df['eps_original'] = df['eps']  # Guardar original para auditor√≠a
        df['eps'] = df['eps'].apply(normalizar_eps_inteligente)
        eps_validas = df['eps'].notna().sum()
        eps_unicas = df['eps'].nunique()
        print(f"   ‚úì EPS: {eps_validas} v√°lidas, {eps_unicas} √∫nicas")
    
    # Municipios
    if 'municipio' in df.columns:
        df['municipio_original'] = df['municipio']  # Guardar original
        df['municipio'] = df['municipio'].apply(normalizar_municipio_inteligente)
        mun_validos = df['municipio'].notna().sum()
        mun_unicos = df['municipio'].nunique()
        print(f"   ‚úì Municipios: {mun_validos} v√°lidos, {mun_unicos} √∫nicos")
    
    # Fechas
    if 'fecha_ingreso' in df.columns:
        df['fecha_ingreso'] = df.apply(
            lambda row: reconstruir_fecha(
                row['fecha_ingreso'],
                row.get('source_file', ''),
                row.get('year_folder', '')
            ), axis=1
        )
        print(f"   ‚úì Fechas ingreso: {df['fecha_ingreso'].notna().sum()}")
    
    if 'fecha_egreso' in df.columns:
        df['fecha_egreso'] = df.apply(
            lambda row: reconstruir_fecha(
                row['fecha_egreso'],
                row.get('source_file', ''),
                row.get('year_folder', '')
            ), axis=1
        )
        print(f"   ‚úì Fechas egreso: {df['fecha_egreso'].notna().sum()}")

    # Recuperaci√≥n por Cruce de Fechas (1900 fix)
    def corregir_por_cruce_fechas(row):
        fi = str(row.get('fecha_ingreso', ''))
        fe = str(row.get('fecha_egreso', ''))
        
        # Caso A: Ingreso es 1900
        if fi.startswith('1900-'):
            if not fe.startswith('1900-') and fe != 'None' and len(fe) > 4:
                year_correct = fe[:4]
            else:
                year_correct = '2019'
            row['fecha_ingreso'] = year_correct + fi[4:]
            
        # Caso B: Egreso es 1900
        elif fe.startswith('1900-'):
            if not fi.startswith('1900-') and fi != 'None' and len(fi) > 4:
                year_correct = fi[:4]
            else:
                year_correct = '2019'
            row['fecha_egreso'] = year_correct + fe[4:]
            
        return row

    if 'fecha_ingreso' in df.columns and 'fecha_egreso' in df.columns:
        print("   ‚úì Recuperando fechas 1900 por cruce...")
        df = df.apply(corregir_por_cruce_fechas, axis=1)
    
    # Sesiones
    if 'sesiones' in df.columns:
        df['sesiones'] = df['sesiones'].apply(limpiar_sesiones)
        print(f"   ‚úì Sesiones: {(df['sesiones'] > 0).sum()}")
    
    # Otros campos
    for col in ['nombres', 'apellidos', 'direccion', 'telefono', 'profesional',
                'observaciones', 'diagnostico', 'tipo_terapia']:
        if col in df.columns:
            df[col] = df[col].apply(limpiar_texto)
    
    # Estad√≠sticas DESPU√âS
    print("\n5. DESPU√âS de limpieza:")
    print(f"   - EPS √∫nicas: {df['eps'].nunique()}")
    print(f"   - Municipios √∫nicos: {df['municipio'].nunique()}")
    print(f"   - Fechas ingreso: {df['fecha_ingreso'].notna().sum()}")
    print(f"   - Sesiones v√°lidas: {(df['sesiones'] > 0).sum()}")
    
    # Identificar registros con fecha 1900 (antes de filtrar por EPS/Municipio)
    def has_1900(row):
        for col in ['fecha_ingreso', 'fecha_egreso']:
            if col in row and str(row[col]).startswith('1900-'):
                return True
        return False
    
    df_1900 = df[df.apply(has_1900, axis=1)].copy()
    
    # Separar registros v√°lidos e inv√°lidos
    print("\n6. Separando registros v√°lidos e inv√°lidos...")
    
    # Registros v√°lidos: tienen EPS Y municipio v√°lidos Y NO tienen fecha 1900
    mask_1900 = df.apply(has_1900, axis=1)
    df_validos = df[(df['eps'].notna()) & (df['municipio'].notna()) & (~mask_1900)].copy()
    
    # Registros rechazados: EPS o municipio inv√°lido
    df_rechazados = df[(df['eps'].isna()) | (df['municipio'].isna())].copy()
    
    print(f"   ‚úì V√°lidos: {len(df_validos)} registros")
    print(f"   ‚úì Con fecha 1900: {len(df_1900)} registros (audit/registros_FECHA_1900.json)")
    print(f"   ‚úì Rechazados: {len(df_rechazados)} registros")
    
    # Guardar registros v√°lidos
    print("\n7. Guardando registros v√°lidos...")
    df_validos_clean = df_validos.where(pd.notna(df_validos), None)
    records_validos = df_validos_clean.to_dict('records')
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(records_validos, f, indent=2, ensure_ascii=False)
    print(f"   ‚úì {output_file}")
    
    # 8. Guardar registros de auditor√≠a
    print("\n8. Guardando registros de auditor√≠a...")
    
    # 8.1 Registros rechazados (EPS/Municipio inv√°lido)
    with open('data/audit/registros_RECHAZADOS.json', 'w', encoding='utf-8') as f:
        df_rechazados_to_save = df_rechazados.where(pd.notna(df_rechazados), None).to_dict('records')
        
        # Agregar raz√≥n del rechazo si no existe
        for record in df_rechazados_to_save:
            if not record.get('razon_rechazo'):
                razones = []
                if not record.get('eps'):
                    razones.append(f"EPS inv√°lida: {record.get('eps_original', 'N/A')}")
                if not record.get('municipio'):
                    razones.append(f"Municipio inv√°lido: {record.get('municipio_original', 'N/A')}")
                record['razon_rechazo'] = ' | '.join(razones)
                
        json.dump(df_rechazados_to_save, f, indent=2, ensure_ascii=False)
    print(f"   ‚úì data/audit/registros_RECHAZADOS.json")
    
    # 8.2 Registros con fecha 1900
    with open('data/audit/registros_FECHA_1900.json', 'w', encoding='utf-8') as f:
        df_1900_to_save = df_1900.where(pd.notna(df_1900), None).to_dict('records')
        for record in df_1900_to_save:
            record['razon_rechazo'] = "A√±o inv√°lido (1900) detectado en fecha ingreso/egreso"
        json.dump(df_1900_to_save, f, indent=2, ensure_ascii=False)
    print(f"   ‚úì data/audit/registros_FECHA_1900.json")
    
    # Reporte detallado
    print("\n9. Generando reporte...")
    
    report = {
        'fecha_limpieza': datetime.now().isoformat(),
        'registros_procesados': len(df),
        'registros_validos': len(df_validos),
        'registros_rechazados': len(df_rechazados),
        'eps': {
            'antes': eps_antes,
            'despues': int(df_validos['eps'].nunique()),
            'top_10': df_validos['eps'].value_counts().head(10).to_dict()
        },
        'municipios': {
            'antes': mun_antes,
            'despues': int(df_validos['municipio'].nunique()),
            'top_10': df_validos['municipio'].value_counts().head(10).to_dict()
        },
        'fechas': {
            'ingreso_validas': int(df_validos['fecha_ingreso'].notna().sum()),
            'egreso_validas': int(df_validos['fecha_egreso'].notna().sum())
        },
        'sesiones_validas': int((df_validos['sesiones'] > 0).sum()),
        'rechazos_por_razon': {
            'eps_invalida': int((df['eps'].isna()).sum()),
            'municipio_invalido': int((df['municipio'].isna()).sum())
        }
    }
    
    with open('data/audit/reporte_limpieza.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"   ‚úì data/audit/reporte_limpieza.json")
    
    print("\n" + "="*80)
    print("‚úÖ LIMPIEZA COMPLETADA")
    print("="*80)
    print(f"\nResultados:")
    print(f"  üìä Total procesados: {len(df):,}")
    print(f"  ‚úÖ V√°lidos: {len(df_validos):,} ({len(df_validos)/len(df)*100:.1f}%)")
    print(f"  ‚ö†Ô∏è  Rechazados: {len(df_rechazados):,} ({len(df_rechazados)/len(df)*100:.1f}%)")
    print(f"\nMejoras:")
    print(f"  EPS: {eps_antes} ‚Üí {df_validos['eps'].nunique()} √∫nicas")
    print(f"  Municipios: {mun_antes} ‚Üí {df_validos['municipio'].nunique()} √∫nicos")
    print(f"\nArchivos generados:")
    print(f"  1. {output_file} - Datos limpios y v√°lidos")
    print(f"  2. data/audit/registros_RECHAZADOS.json - Para revisi√≥n manual")
    print(f"  3. {backup_file} - Backup original")
    print()

if __name__ == "__main__":
    limpiar_datos_maestro()
